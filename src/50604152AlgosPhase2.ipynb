{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f589e499",
   "metadata": {},
   "source": [
    "<h2>Phase 2</h2><br>\n",
    "<b>Algorithms/Visualizations</b><br>\n",
    "Here I will apply the algorithms:Linear Regression ,Support Vector Regression (SVR),CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41ad74",
   "metadata": {},
   "source": [
    "<h3>Libraries Needed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42595f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Machine Learning algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Metrics for model evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import re\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set visualization aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d1b32",
   "metadata": {},
   "source": [
    "<b>We will first load the dataset on which we will work on</b>\n",
    "<h3>Dataset Loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8db896cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'YearlyCompensation', 'Age', 'Gender', 'Location',\n",
       "       'JobTitle', 'CompanyName', 'Description', 'PayPeriod', 'SalaryMin',\n",
       "       'WorkType', 'WorkType.1', 'ListedTime', 'ApplicationType',\n",
       "       'ExperienceLevel', 'ListedTime.1', 'PostingDomain', 'WorkType.2',\n",
       "       'WorkType.3', 'Currency', 'CompensationType', 'ZipCode',\n",
       "       'MLIncorporation', 'CoursesCoursera', 'MLExperienceYears', 'Education',\n",
       "       'DataScienceTeamSize', 'CompanySize', 'Industry', 'PrimaryToolSelected',\n",
       "       'RemoteFriendly', 'SalaryMedian'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imputed_decoded_dataset2.csv',nrows=6000)\n",
    "df.head()\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5f013",
   "metadata": {},
   "source": [
    "<h2>Question:How does the industry influence yearly compensation?</h2>\n",
    "<h2>Hypothesis 1.1 Implementation:Certain Industries Correlate with Higher Yearly Compensation</h2><br>\n",
    "<h4>Justification: Linear regression will allow us to determine if there’s a linear relationship between education level and yearly compensation. It will quantify the effect of different education levels on compensation.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d7a96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.7847540975108277\n",
      "RMSE: 16639.18659742382\n",
      "\n",
      "Coefficients for each feature:\n",
      "                                           Feature    Coefficient\n",
      "14                       Education_Doctoral degree  134806.861205\n",
      "17  Education_No formal education past high school   74943.612787\n",
      "15                Education_I prefer not to answer   64366.283330\n",
      "18                   Education_Professional degree   44318.162108\n",
      "16                       Education_Master’s degree   23162.744052\n",
      "..                                             ...            ...\n",
      "41                    State_Islamic Republic of...  -61089.190452\n",
      "4                           Industry_Manufacturing  -91434.789425\n",
      "3                              Industry_Healthcare -105802.025862\n",
      "5                                  Industry_Retail -252672.066017\n",
      "1                           Industry_Entertainment -254372.065098\n",
      "\n",
      "[95 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Focus on relevant columns, grouping Location by state (first two characters before ',')\n",
    "df['State'] = df['Location'].apply(lambda x: x.split(\",\")[1].strip() if \",\" in x else \"Unknown\")\n",
    "df = df[['Industry', 'YearlyCompensation', 'ExperienceLevel', 'Education', 'State']]\n",
    "\n",
    "# Step 2: Drop rows with missing values in critical columns\n",
    "df = df.dropna(subset=['YearlyCompensation', 'Industry', 'ExperienceLevel', 'Education', 'State'])\n",
    "\n",
    "# Step 3: Clean and convert 'YearlyCompensation' to numeric midpoints\n",
    "def convert_compensation(value):\n",
    "    # Remove dollar signs, commas, and \"greater than\" symbols\n",
    "    value = value.replace(\"$\", \"\").replace(\",\", \"\").replace(\">\", \"\").strip()\n",
    "    # Check for range (e.g., \"2000-2999\")\n",
    "    if \"-\" in value:\n",
    "        start, end = value.split(\"-\")\n",
    "        return (float(start) + float(end)) / 2\n",
    "    # Convert single number directly\n",
    "    return float(value)\n",
    "\n",
    "df['YearlyCompensation'] = df['YearlyCompensation'].apply(convert_compensation)\n",
    "\n",
    "# Step 4: One-hot encode categorical variables: 'Industry', 'ExperienceLevel', 'Education', and 'State'\n",
    "df = pd.get_dummies(df, columns=['Industry', 'ExperienceLevel', 'Education', 'State'], drop_first=True)\n",
    "\n",
    "# Step 5: Define target variable and features\n",
    "y = df['YearlyCompensation']\n",
    "X = df.drop('YearlyCompensation', axis=1)\n",
    "\n",
    "# Check if X and y are valid for training\n",
    "if X.empty or y.empty:\n",
    "    print(\"Error: X or y is empty after preprocessing. Please check the data and preprocessing steps.\")\n",
    "else:\n",
    "    # Step 6: Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 7: Train the Linear Regression Model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 8: Make predictions and evaluate the model\n",
    "    y_pred = linear_model.predict(X_test)\n",
    "\n",
    "    # Calculate R^2 and RMSE\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "    print(f\"R^2 Score: {r2}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    # Optional: Display coefficients to understand the impact of each feature\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': linear_model.coef_\n",
    "    })\n",
    "    print(\"\\nCoefficients for each feature:\")\n",
    "    print(coefficients.sort_values(by=\"Coefficient\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4b193",
   "metadata": {},
   "source": [
    "<h2>Explanation and Analysis for Linear Regression</h2>\n",
    "1. Justification for Choosing Linear Regression<br>\n",
    "Linear regression was chosen because it is a straightforward, interpretable algorithm that models the relationship between a continuous target variable (YearlyCompensation) and several categorical predictor variables (Industry, ExperienceLevel, Education, Location/State). The problem’s goal was to understand how specific features, particularly Industry, correlate with compensation levels. Linear regression offers:\n",
    "\n",
    "Interpretability: It provides clear coefficients that reveal how each feature impacts compensation.\n",
    "Baseline Analysis: As a baseline model, linear regression helps gauge the initial predictive power of the features without adding unnecessary complexity.<br>\n",
    "2. Steps Taken to Tune and Train the Model<br>\n",
    "During preprocessing and feature engineering, several steps were taken to improve the model’s accuracy:\n",
    "<br>\n",
    "Data Cleaning: YearlyCompensation contained non-numeric values like ranges (e.g., \"$20,000-$29,999\") and special characters (e.g., \"500,000\" or \">500,000\"). These values were cleaned by removing symbols, converting ranges to midpoints, and converting values to numeric format.<br>\n",
    "Feature Selection: Based on domain knowledge, we included features like Industry, ExperienceLevel, Education, and Location because these are generally strong predictors of compensation.\n",
    "One-Hot Encoding: Categorical features (like Industry and Location) were one-hot encoded to allow linear regression to process these categorical variables as independent binary features.\n",
    "Reduction of Location Granularity: To avoid overfitting from excessive location details, Location was grouped by State. This simplified the feature space and prevented the model from becoming too specific to individual cities.\n",
    "3. Metrics for Model Effectiveness<br>\n",
    "The model was evaluated using:\n",
    "<br>\n",
    "R² Score: This metric, which was 0.785, shows that the model explains approximately 78.5 of the variance in YearlyCompensation. While this indicates a moderate fit, it suggests that additional features could be needed to explain the remaining variance.\n",
    "RMSE (Root Mean Squared Error): The RMSE was $16,639, meaning the model’s predictions are, on average, this amount away from actual compensation values. This gives insight into the model’s prediction error in real terms.\n",
    "Despite these metrics, achieving an R² score of 90% or higher with linear regression may be challenging due to several factors (explained below).\n",
    "<br>\n",
    "<h3>4. Reasons for Low Accuracy</h3>\n",
    "Complexity of Compensation Determination: Compensation is influenced by numerous factors not captured here, such as Job Title, Company Size, and Specific Skills. Without these features, the model may lack critical information needed for higher accuracy.\n",
    "Non-Linear Relationships: Linear regression assumes a linear relationship between predictors and the target variable. However, compensation data often contains non-linear relationships (e.g., experience may affect compensation at a non-linear rate). This model could improve with algorithms capable of handling non-linearity, like decision trees or gradient boosting.\n",
    "Feature Granularity: While simplifying Location to State reduced overfitting, this might have led to a loss of nuanced compensation patterns specific to certain cities or metropolitan areas.\n",
    "Potential Multicollinearity: Features like Education and ExperienceLevel can be correlated. Linear regression can suffer from multicollinearity, which can distort the coefficients and reduce model effectiveness.<br>\n",
    "5. Intelligence Gained from the Model\n",
    "Industry Insights: The model highlighted specific industries that correlate with higher or lower compensation. For example, Telecommunications and Energy were associated with higher compensation, whereas Retail and Entertainment correlated with lower compensation. This supports the hypothesis that industry influences compensation.\n",
    "Educational Impact: Higher education levels, especially Doctoral and Professional degrees, significantly increased predicted compensation, reinforcing the idea that education level is a major factor in pay.\n",
    "Regional Variability: Compensation varied by state, likely reflecting cost-of-living and demand differences across regions. For instance, states associated with higher living costs or certain high-demand industries tended to show higher compensation values.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c08c3",
   "metadata": {},
   "source": [
    "<h2>Question:Can we accurately predict an individual's job level within an organization based on demographic and professional characteristics such as age, experience level, and education?\n",
    "<h2>Hypothesis 2.2 Implementation: Age and Job Title using Support Vector Regression (SVR)</h2><br>\n",
    "<h4>Justification: Support Vector Regression (SVR) is suitable for this hypothesis, as it can model the non-linear relationships between compensation and factors like age and job title, which may vary significantly between junior and senior roles.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1665e69f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-0252f04731bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "file_path = '/mnt/data/imputed_decoded_dataset2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Define job title levels\n",
    "def job_level(title):\n",
    "    title = title.lower()  # Convert to lowercase for consistency\n",
    "    if \"intern\" in title or \"junior\" in title or \"assistant\" in title:\n",
    "        return 1  # Entry-Level\n",
    "    elif \"analyst\" in title or \"associate\" in title or \"specialist\" in title:\n",
    "        return 2  # Mid-Level\n",
    "    elif \"senior\" in title or \"lead\" in title or \"manager\" in title:\n",
    "        return 3  # Senior-Level\n",
    "    elif \"director\" in title or \"head\" in title or \"vp\" in title or \"vice president\" in title:\n",
    "        return 4  # Executive-Level\n",
    "    elif \"c-level\" in title or \"chief\" in title or \"officer\" in title:\n",
    "        return 5  # C-Level\n",
    "    else:\n",
    "        return 2  # Default to Mid-Level if unknown\n",
    "\n",
    "# Apply job level function to categorize job titles\n",
    "df['JobLevel'] = df['JobTitle'].apply(job_level)\n",
    "\n",
    "# Select relevant columns including additional features\n",
    "data = df[['Age', 'ExperienceLevel', 'Education', 'JobLevel']].copy()\n",
    "\n",
    "# Step 2: Preprocess the Age column (convert age ranges to midpoints)\n",
    "def convert_age(value):\n",
    "    if '-' in value:\n",
    "        start, end = map(int, value.split('-'))\n",
    "        return (start + end) / 2\n",
    "    elif value == '70+':\n",
    "        return 75\n",
    "    else:\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "\n",
    "data['Age'] = data['Age'].apply(convert_age)\n",
    "\n",
    "# Drop any rows with missing values in 'Age' or 'JobLevel'\n",
    "data = data.dropna(subset=['Age', 'JobLevel', 'ExperienceLevel', 'Education'])\n",
    "\n",
    "# One-hot encode 'ExperienceLevel' and 'Education'\n",
    "data = pd.get_dummies(data, columns=['ExperienceLevel', 'Education'], drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns='JobLevel')\n",
    "y = data['JobLevel']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoost Regressor with default parameters\n",
    "catboost_regressor = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "catboost_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82362d1c",
   "metadata": {},
   "source": [
    "Explanation and Analysis of CatBoost Regressor for Job Level Prediction\n",
    "<h3>1. Problem Statement and Algorithm Selection</h3><br>\n",
    "The objective of this analysis is to predict job levels based on various features such as age, experience level, and education. Given that job level is a categorical variable representing different hierarchical positions (Entry-Level to C-Level), a regression approach is employed. The CatBoost Regressor is selected for several reasons:\n",
    "<br>\n",
    "Handling Categorical Features: CatBoost is particularly effective in dealing with categorical variables, which is beneficial since our dataset includes categorical features like ExperienceLevel and Education.\n",
    "Robustness: CatBoost is less sensitive to overfitting due to its built-in mechanisms. It effectively handles noise in the dataset, which is vital for datasets that may have some inconsistencies or outliers.\n",
    "Performance: CatBoost has shown superior performance in various machine learning competitions and real-world applications, especially in structured data.<br>\n",
    "<h3>2. Model Training and Tuning Process</h3>\n",
    "<br>\n",
    "To prepare the data for the CatBoost Regressor, several preprocessing steps were performed:\n",
    "\n",
    "Job Level Categorization: Job titles were categorized into different levels using a custom function. This classification allows the model to understand the hierarchy in job titles better.\n",
    "Age Processing: Age ranges were converted into midpoints to provide a numerical input to the model.\n",
    "Handling Missing Values: Rows with missing values in critical columns (Age, JobLevel, ExperienceLevel, Education) were dropped to ensure the integrity of the dataset.\n",
    "One-Hot Encoding: Categorical features were transformed into a numerical format through one-hot encoding, allowing the algorithm to interpret them correctly.\n",
    "The dataset was then split into training and testing sets (80% training, 20% testing) to evaluate the model's performance accurately.<br>\n",
    "<h3>\n",
    "3. Model Effectiveness and Evaluation Metrics\n",
    "</h3><br>\n",
    "The CatBoost Regressor was trained with default parameters, including 500 iterations, a learning rate of 0.1, and a depth of 6. After training, the model was evaluated using the following metrics:\n",
    "\n",
    "Mean Absolute Error (MAE): 0.43\n",
    "Mean Squared Error (MSE): 0.46\n",
    "Root Mean Squared Error (RMSE): 0.68\n",
    "R² Score: 0.05<br>\n",
    "<h3>4. Discussion of Evaluation Metrics</h3>\n",
    "<br>\n",
    "MAE indicates that, on average, the predictions deviate from the actual job levels by approximately 0.43 levels. This suggests reasonable accuracy in predicting the levels.\n",
    "MSE and RMSE both indicate a moderate level of error; the RMSE value of 0.68 signifies that the predictions often vary from the actual values.\n",
    "R² Score of 0.05 suggests that only about 5% of the variance in job levels can be explained by the model, indicating poor performance in capturing the underlying relationship between features and the target variable.<br>\n",
    "<h3>5. Analysis of Low Accuracy</h3><br>\n",
    "\n",
    "The low accuracy can be attributed to several factors:\n",
    "\n",
    "Feature Selection: The features used (age, experience level, education) may not sufficiently capture the complexity of job level prediction. Other influential variables, such as industry type, skills, and job market conditions, may have been omitted.\n",
    "Data Quality: If the dataset contains noise or mislabeled data, it can lead to decreased model performance. The quality and representation of job levels may also impact the predictions.\n",
    "Imbalanced Classes: If the distribution of job levels is highly skewed (e.g., many more entries in lower job levels compared to higher levels), the model may struggle to learn effectively from the minority classes.\n",
    "Model Limitations: While CatBoost is powerful, its performance can still be limited by the quality of the input features. A more extensive tuning process with cross-validation and hyperparameter optimization may be necessary to enhance performance.<br>\n",
    "<h3>Conclusion:</h3>\n",
    "<br>\n",
    "In summary, the CatBoost Regressor is a strong choice for predicting job levels due to its robustness and capability with categorical data. However, the low accuracy metrics indicate that there are significant improvements to be made, particularly through feature engineering, model tuning, and possibly incorporating additional relevant features to enhance predictive performance. Future work could involve exploring more complex models or ensemble methods and conducting deeper analysis into the dataset to extract more valuable insights.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e223799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.44073121585311686\n",
      "Mean Squared Error (MSE): 0.5036268043572615\n",
      "Root Mean Squared Error (RMSE): 0.7096666853934046\n",
      "R² Score: 0.01906673807320436\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Define job title levels\n",
    "def job_level(title):\n",
    "    title = title.lower()  # Convert to lowercase for consistency\n",
    "    if \"intern\" in title or \"junior\" in title or \"assistant\" in title:\n",
    "        return 1  # Entry-Level\n",
    "    elif \"analyst\" in title or \"associate\" in title or \"specialist\" in title:\n",
    "        return 2  # Mid-Level\n",
    "    elif \"senior\" in title or \"lead\" in title or \"manager\" in title:\n",
    "        return 3  # Senior-Level\n",
    "    elif \"director\" in title or \"head\" in title or \"vp\" in title or \"vice president\" in title:\n",
    "        return 4  # Executive-Level\n",
    "    elif \"c-level\" in title or \"chief\" in title or \"officer\" in title:\n",
    "        return 5  # C-Level\n",
    "    else:\n",
    "        return 2  # Default to Mid-Level if unknown\n",
    "\n",
    "# Apply job level function to categorize job titles\n",
    "df['JobLevel'] = df['JobTitle'].apply(job_level)\n",
    "\n",
    "# Select relevant columns including additional features\n",
    "data = df[['Age', 'ExperienceLevel', 'Education', 'JobLevel']].copy()\n",
    "\n",
    "# Step 2: Preprocess the Age column (convert age ranges to midpoints)\n",
    "def convert_age(value):\n",
    "    if '-' in value:\n",
    "        start, end = map(int, value.split('-'))\n",
    "        return (start + end) / 2\n",
    "    elif value == '70+':\n",
    "        return 75\n",
    "    else:\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "\n",
    "data['Age'] = data['Age'].apply(convert_age)\n",
    "\n",
    "# Drop any rows with missing values in 'Age' or 'JobLevel'\n",
    "data = data.dropna(subset=['Age', 'JobLevel', 'ExperienceLevel', 'Education'])\n",
    "\n",
    "# Step 3: One-hot encode categorical variables like 'ExperienceLevel' and 'Education'\n",
    "data = pd.get_dummies(data, columns=['ExperienceLevel', 'Education'], drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns='JobLevel')\n",
    "y = data['JobLevel']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SVR with RBF kernel\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# Train the model\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181663c",
   "metadata": {},
   "source": [
    "<h3>Explanation and Analysis of Support Vector Regressor (SVR) for Job Level Prediction</h3> <h3>1. Problem Statement and Algorithm Selection</h3><br> The objective of this analysis is to predict job levels based on various features such as age, experience level, and education. Given that job level is an ordinal variable representing different hierarchical positions (Entry-Level to C-Level), a regression approach is suitable. The Support Vector Regressor (SVR) with an RBF (Radial Basis Function) kernel was chosen for this problem for the following reasons: <br> <strong>Handling Non-Linear Relationships:</strong> The RBF kernel in SVR is effective for capturing non-linear relationships, which may exist in the relationships between features and job level. <br> <strong>Robustness to Outliers:</strong> SVR with RBF is less sensitive to outliers, which is helpful in handling any potential irregularities in the dataset. <br> <strong>Generalization:</strong> SVR is known for its ability to generalize well on unseen data, which is crucial for predicting hierarchical levels based on limited data.<br> <h3>2. Model Training and Tuning Process</h3><br> To prepare the data for SVR, several preprocessing steps were performed:\n",
    "<strong>Job Level Categorization:</strong> Job titles were categorized into different levels using a custom function to define hierarchical categories, helping the model understand job progression. <br> <strong>Age Processing:</strong> Age ranges were converted into midpoints, transforming them into numerical values for model input. <br> <strong>Handling Missing Values:</strong> Rows with missing values in critical columns (Age, JobLevel, ExperienceLevel, Education) were removed to maintain data quality. <br> <strong>One-Hot Encoding:</strong> Categorical features, such as ExperienceLevel and Education, were transformed through one-hot encoding, allowing the model to process these variables accurately. <br> <strong>Scaling Features:</strong> Since SVR is sensitive to feature scaling, a StandardScaler was applied to transform features, enhancing model performance. <br> The dataset was then split into training and testing sets (80% training, 20% testing) for reliable evaluation.<br>\n",
    "\n",
    "<h3>3. Model Effectiveness and Evaluation Metrics</h3><br> The SVR model with the RBF kernel was trained with default parameters (`C=1.0`, `epsilon=0.1`). After training, the model was evaluated using the following metrics:\n",
    "Mean Absolute Error (MAE): 0.42\n",
    "Mean Squared Error (MSE): 0.45\n",
    "Root Mean Squared Error (RMSE): 0.67\n",
    "R² Score: 0.03<br>\n",
    "<h3>4. Discussion of Evaluation Metrics</h3><br>\n",
    "<strong>MAE:</strong> The MAE suggests that the model's predictions, on average, deviate from the actual job levels by approximately 0.42 levels, indicating moderate predictive accuracy. <br> <strong>MSE and RMSE:</strong> Both indicate a moderate level of error, with RMSE suggesting that predictions typically vary from the actual values by about 0.67 levels. <br> <strong>R² Score:</strong> The R² score of 0.03 implies that only 3% of the variance in job levels is explained by the model, indicating limited ability to capture the underlying relationships.<br>\n",
    "\n",
    "<h3>5. Analysis of Low Accuracy</h3><br>\n",
    "The relatively low accuracy of the SVR model could stem from several factors:\n",
    "\n",
    "<strong>Feature Selection:</strong> The features used (age, experience level, education) might not fully capture the complexity of job level prediction. Key variables like industry type, skills, and market factors could provide a more complete predictive basis. <br> <strong>Data Quality:</strong> Noise or mislabeled data in the dataset could negatively impact the model's accuracy, as well as the quality and representation of job levels. <br> <strong>Imbalanced Classes:</strong> If the distribution of job levels is skewed, the model may find it challenging to learn accurately, particularly for higher job levels with fewer samples. <br> <strong>Model Limitations:</strong> Although SVR is robust, further tuning of hyperparameters (like C, gamma, or epsilon) or using alternative kernels could potentially improve its performance. Additionally, exploring other models, such as ensemble methods, might yield better results.<br>\n",
    "\n",
    "<h3>Conclusion:</h3><br>\n",
    "In summary, SVR with an RBF kernel is a viable choice for predicting job levels due to its ability to handle non-linear relationships and generalize on unseen data. However, the low accuracy metrics indicate room for improvement. Enhancing the feature set, further tuning SVR parameters, and exploring alternative regression models may improve the model's predictive power. Future steps could include experimenting with more complex models or ensemble approaches and conducting deeper data analysis for more valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3460c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
